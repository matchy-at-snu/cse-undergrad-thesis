\chapter{Materials \& Methods}

\section{Keypair Generation Algorithm}

Different from a regular RSA key generation process, where $N = pq$, $p$ and $q$ are prime numbers, a distributed modulo has

\begin{equation}
  N = \left( \sum_1^k p_i\right) \left( \sum_1^k q_i \right)
\end{equation}

During the computation, the value of each $p_i$ and $q_i$ is not explicitly revealed among workers.

A method described by D. Boneh and M. Franklin (citation) is used. The process is further divided into modulo generation, primality test and key generation.

\subsection{Modulus Generation}

One prime number $P$ satisfying $P > N$ is sent to all workeres.

Let $l = \left\lfloor \frac{k-1}{2} \right\rfloor $, $\forall i = 1, ..., k$, worker $i$ picks two random degree $l$ polynomials $f_i, g_i \in \mathbb{Z}_P[x]$ satisfying $f_i(0) = p_i$, $g_i(0) = q_i$, and a random degree $2l$ polynomial $h_i \in \mathbb{Z}_P[x]$ satisfying $h_i(0) = 0$.

For all $i = 1, ..., k$, each worker $i$ computes:

\begin{equation}
\begin{split}
  \forall j = 1, ..., k, \quad & p_{i, j} = f_i(j) \\
  & q_{i, j} = g_i(j) \\
  & h_{i, j} = h_i(j)
\end{split}
\end{equation}

Worker $i$ then privately sends the tuple $\left\langle p_{i,j}, q_{i,j}, h_{i,j}\right\rangle$ to server $j$ for all $j \neq i$.

After the communication, each worker $i$ has all of $\left\langle p_{i,j}, q_{i,j}, h_{i,j}\right\rangle, \quad \forall j = 1, ..., k$. Worker $i$ then computes:

\begin{equation}
    N_i = \left( \sum_{j=1}^{k} p_{j, i}\right)\left( \sum_{j=1}^{k} q_{j, i}\right) + \sum_{j=1}^{k} h_{j,1} (\mod P)
\end{equation}

After the computation, $N_i$ is broadcast to all other workers. Now, each worker $j$ has all values of $N_i$ for $i = 1,..., k$.

Let $\alpha(x)$ be the polynomial

\begin{equation}
  \alpha(x) = \left( \sum_j f_j(x)\right) \left( \sum_j g_j(x)\right) + \sum_j h_j(x) (\mod P)
\end{equation}

Observe that $\alpha(i) = N_i$ and by definition of $f_i$, $g_i$ and $h_i$, we have $\alpha(0) = N$. Furthermore, $\alpha(x)$ is a polynomial of degree $2l$. We note that $l$ is defined in such at way so that $k \leq 2l + 1$. Since all servers have at least $2l + 1$ points on $\alpha(x)$, they can interpolate the points to obtain the coefficients of $\alpha(x)$. Each server finally evaluates $\alpha(0)$ using the interpolated coefficients and obtains $N \mod P$. Since $N < P$, each server learns the correct value of $N$ without disclosing any information.

Note that in the above process $p_{i,j}$ and $q_{i,j}$ are the standard l-out-of-k Shamir sharing of $p_i$ and $q_i$.

\subsection{Primality Test of Modulus}

A random number $g \in \mathbb{Z}_N^*$ is chosen and broadcast to all $k$ workers.

Worker 1 (not necessarily the first worker, just selected to be the "primary" worker) computes:

\begin{equation}
  v_1 = g^{N - p_1 - q_1 + 1}
\end{equation}

All other workers compute

\begin{equation}
  v_i = g^{p_i + q_i}
\end{equation}

The workers then exchange the $v_i$ values with each other and verify that

\begin{equation}
  v_1 = \prod_{i=2}^{k} v_i \quad (\mod N)
\end{equation}

The test is referred as Fermat test for testing that a number is a product of two primes. It is possible that for a $N$ that is not a product of two primes to pass the test, but the density is significantly small (less than $1$ out of $10^{40}$).

\subsection{Shared Generation of Private Keys}

For distributed generation of private keys, we use the method introduced by Dario Catalano, Rosario Gennaro and Shai Halevi for its simplicity and security under the
assumed condition we set.

A number $e$ is chosen and known to all workers.

Worker 1 (not necessarily the first worker, just selected to be the "primary" worker) locally computes:

\begin{equation}
  \Phi_1 = N - p_1 - q_1 + 1
\end{equation}

All other workers copmute

\begin{equation}
  \Phi_i = - p_i - q_i
\end{equation}

It's obvious that

\begin{equation}
  N = \sum_i \Phi_i, \quad i = 1, ..., k
\end{equation}

Then, each worker computes

\begin{equation}
  \gamma_i = r_i \cdot e + \Phi_i
\end{equation}

where $r_i$ is a random number decided by the worker. $\gamma_i$ is then shared among all workers. And then all workers can compute:

\begin{equation}
  \gamma = \sum_i \gamma_i = \Phi + R \cdot e
\end{equation}

There are $a$ and $b$ such that

\begin{equation}
  a \cdot \gamma + b \cdot e = 1 \\
  d = a \cdot R + b
\end{equation}

Then we can compute

\begin{equation}
  a = \gamma^{-1} (\mod e), \qquad b = \frac{1-a \cdot \gamma}{e}
\end{equation}

With all the information each worker holds, a usable additive share of $d$ can be:

\begin{equation}
  d_1 = a\cdot r_1 + b, \qquad d_{i \neq 1} = a \cdot r_i
\end{equation}

\section{Message Encryption and Decryption Algorithm}

In order to communiate with other application of web, we select to use \texttt{RSAES-PKCS1-V1\_5-ENCRYPT} scheme. This scheme uses both RSA and ECB algorithm. First, we split the message to encrypt into $n$ blocks, and then do $RSA$ encryption to each of it. This scheme includes 11 bytes padding, so preprocessing is required for each block. The first two bytes of the padding are \texttt{0x00} and \texttt{0x02}, followed by a randomly generateed 8-byte octet string. The last byte of the padding is \texttt{0x00}. The original message was attached at the end of the padding.

After preprocessing, we do RSA encryption block by block. Let block of string $EM$, then the encrypted message $C$ is

\begin{equation}
  C = EM^e \mod N
\end{equation}

And we combine $C$ to make the whole encrypted message $M$.

To decrypt this message $M$, the worker get the input encrypted message $M$ and computes

\begin{equation}
  s_i = m^{d_i} \mod N
\end{equation}

The result (called \textit{shadow}) is sent back to the receiver (manager), and manager computes

\begin{equation}
  s = \prod_i s_i = m^{\sum_i d_i} = m^d \mod N
\end{equation}

Thus, the valid decrypted message is obtained.

\section{Optimizations}

Many optimizations are applied in various aspects in this project, but the contributions of most of them are either difficult to measure or optimized for better memory consumption or network bandwidth consumption. The most conspicuous and important optimizations in the aspect of speeding up the running speed of this application are the two listed below: distributed sieving and parallel modulus generation.

\subsection{Distributed Sieving Algorithm}

In section (), we set $p_i$ and $q_i$ to be random prime numbers and use a primality test to verify whether $N = \left( \sum_1^k p_i\right) \left( \sum_1^k q_i \right)$ only has two prime factors. As the generated key bit length increases, the possibility that $N$ satisfies our requirement decreases quadratically, making this method unpractical for real-world application.

Distributed Sieving is an important optimization technique to eliminate small prime factors from the generated $N$ by choosing $p_i$ and $q_i$ wisely.

(unfinished)

\subsection{Parallel Modulus Generation}

The process of modulus generation is a randomized process, even with the help of
sieving, itâ€™s not guaranteed how many attempts will be gone through before a valid
modulus ð‘ is generated.

During the generation, the cluster faces many synchronization points where some
workers must stop computing and wait for other ones to complete the previous
procedure, leaving some of the CPU threads idle or underutilized. The unbalanced
work assignment such as in the primality test, worker 1 is taking significantly more computation job than other workers, makes this problem even worse. A simple work balancing will not help since the cluster always needs to wait for one test holder to complete result checking.

Thus, generating modulus and testing in parallel is of great help. In our implementation, when a parallel flag is given, each worker will host an independent modulus generation-test loop until one of them has found a valid ð‘ , after which all other workers will abort the hosted job and participate the private key generation using the valid ð‘.

To eliminate the interference of parallel workflow, each workflow has a unique workflow ID attached when sending messages to other workers. And the data receiving and waiting mechanism is also optimized for this feature.

Theoretically this optimization will massively increase CPU utilization and although it may potentially decrease the speed of each generation-test workflow, it will prevent the relatively bad cases and thus increase the overall performance. But the final effect differs by the cluster size, worker specs and connection quality.

\section{Implementation}

The application has two different working modes; thus, the architecture differs with different mode.

In worker mode, the application contains two major modules, worker RPC service and worker control module. The worker RPC service manages the communication between other RPC services and is used to form the computation cluster. The worker control module contains the computation logic for key generation and message decryption. Since the worker process does not need to read user console input aside from the starting command line arguments, the worker control module does have any input function.

In manager mode, the application contains two major modules, manager RPC service and manager control logic. The manager RPC service communicates to all other RPC services to form the cluster. The manager control module needs to interact with user through command line input to gain information such as worker nodes URIs.

(figure) shows an example cluster. A cluster includes at least 3 worker nodes and at least 1 manager node. The type of node is determined by the command line argument sent to the application and a client can start work as a worker and a manager simultaneously. The application will start a worker node and/or a manager node and enter the interactive mode of a manager or waiting for a worker to terminate accordingly. The components can be easily transplanted and used in other projects and work in non-interactive mode.

Each worker includes 4 components, the control logic, the RPC sender, the RPC receiver and the data receiver. RPC sender sends RPC request to other workers. Due to the life span of an outgoing RPC call depends on the sending threadâ€™s, an async executor pool for sending requests and optimize it to reduce delays. The RPC receiver receives incoming RPC requests and handles the request to the data receiver or the control logic depending on the RPC type. The workflow is highly parallel, and the information needed for the computation is shared via push messages, thus data receiver uses concurrent maps to store information across different parallel workflows.

Manager only has 3 components, the RPC sender, control logic and data receiver. The way they function is very similar to those of a worker, but a manager does not have an RPC server as it only sends requests to workers.

The example clusters we are showing are made up with a single manager and multiple workers, but the number can vary, and our app also supports a client to run in both manager mode and worker mode simultaneously.

The application has a command line user interface and will start at different mode with different starting arguments given. User needs to assign port number for communication and the IP address needs to be input from console for a process in manager mode.

The command line argument parsing is implemented by hand and requires the user to give information in a fixed format.

The internal implementation of WorkerMain and ManagerMain supports to be used in a non-interactive mode with other code, but the application wrapped around them will always start a manager in interactive mode for demonstration purpose.

In each process, different functional modules are different working phases of one single process, thus no communication interface is needed.

Between processes, the communication is managed by gRPC modules. gRPC is used for defining, sending, and receiving messages, with blocking and unblocking ways.
